# -*- coding: utf-8 -*-
"""Meramal_Cuaca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13GThmufdTcK_rvZ4jIr_-5wvxR6SfVuL

## Business Understanding

Tim peneliti melakukan penelian ini dengan tujuan sebagai berikut :

*   Menemukan hubungan ke empat kolom mengenai `soil_moisture` dengan jumlah curah hujan harian
*   Pasangan kota yang dekat secara geografis
*   Mencari letak daerah berdasarkan kota
*   Mencari 3 kota yang memiliki rata-rata jumlah curah hujan tahunan tertinggi beserta nilainya

## Data Understanding

Dataset Meramal Cuaca (MC) terdiri atas dua data, yaitu `Data Harian` dan `Data Jam`. Data Harian memiliki **14730 baris** dan Data Jam memiliki **353520 baris**.


terdapat **17 kolom** pada data harian dan **27 kolom** pada data jam. detail kolom-kolom tersebut adalah sebagai berikut:

**Data harian:**

* `Id` - id cuaca
* `time` - Tanggal pencatatan
* `temperature_2m_max (°C)` - Temperatur udara tertinggi pada ketinggian 2 m di atas permukaan
* `temperature_2m_min (°C)` - Temperatur udara terendah pada ketinggian 2 m di atas permukaan
* `apparent_temperature_max (°C)` - Temperatur semu maksimum yang terasa
* `apparent_temperature_min (°C)` - Temperatur semu minimum yang terasa
* `sunrise (iso8601)` - Waktu matahariterbit pada hari itu dengan format ISO 8601
* `sunset (iso8601)` - Waktu mataharitenggelam pada hari itu dengan format ISO 8601
* `shortwave_radiation_sum (MJ/m²)` - Total radiasi matahari pada haritersebut
* `rain_sum (mm)` - Jumlah curah hujan pada hari tersebut - Target Regresi
* `snowfall` - Jumlah salju pada haritersebut
* `precipitation` - Jumlah curah hujan harian (termasuk hujan, hujan lebat, dan salju)
* `windspeed_10m_max (km/h)` - Kecepatan angin maksimum pada ketinggian 10 m
* `windgusts_10m_max (km/h)` - Kecepatan angin minimum pada ketinggian 10 m
* `winddirection_10m_dominant (°)` - Arah angin dominan pada haritersebut
* `city` - Nama kota yang tercatat
* `class` - Jenis/kelompok curah hujan harian - Target Klasifikasi

**Data Jam:**
* `time` - Tanggal dan jam pencatatan
* `temperature_2m (°C)` - Temperatur pada ketinggian 2 m
* `relativehumidity_2m (%)` - Kelembapan pada ketinggian 2 m
* `winddirection_100m (°)` - Arah angin pada ketinggian 100 m
* `windgusts_10m (km/h)` - Kecepatan angin ketika terdapat angin kencang
* `et0_fao_evapotranspiration (mm)` - Jumlah evapotranspirasi (evaporasi dan transpirasi) pada jam tersebut
* `vapor_pressure_deficit (kPa)` - Perbedaan tekanan uap air dari udara dengan tekanan uap air ketika udara tersaturasi
* `soil_temperature_0_to_7cm (°C)` - Rata-rata temperatur tanah pada kedalaman 0-7 cm
* `soil_temperature_7_to_28cm (°C)` - Rata-rata temperatur tanah pada kedalaman 7-28 cm
* `soil_temperature_28_to_100cm (°C)` - Rata-rata temperatur tanah pada kedalaman 28-100 cm
* `soil_temperature_100_to_255cm (°C)` - Rata-rata temperatur tanah pada kedalaman 100-255 cm
* `soil_moisture_0_to_7cm (m³/m³)` - Rata-rata kelembapan air pada tanah untuk kedalaman 0-7 cm
* `soil_moisture_7_to_28cm (m³/m³)` - Rata-rata kelembapan air pada tanah untuk kedalaman 7-28 cm
* `dewpoint_2m (°C)` - Titik embun; suhu ambang udara mengembun
* `apparent_temperature (°C)` - Temperatur semu yang dirasakan
* `pressure_msl (hPa)` - Tekanan udara pada ketinggian permukaan air laut rata-rata (mean sea level)
* `surface_pressure (hPa)` - Tekanan udara pada ketinggian permukaan daerah tersebut
* `snowfall (cm)` - Jumlah hujan salju pada jam tersebut
* `cloudcover_low (%)` - Persentase cloud cover pada awan sampai ketinggian 2 km
* `cloudcover_mid (%)` - Persentase cloud cover pada ketinggian 2-6 km
* `cloudcover_high (%)` - Persentase cloud cover pada ketinggian di atas 6 km
* `windspeed_10m (km/h)` - Kecepatan angin pada ketinggian 10 m
* `windspeed_100m (km/h)` - Kecepatan angin pada ketinggian 100 m
* `winddirection_10m (°)` - Arah angin pada ketinggian 10 m
* `soil_moisture_28_to_100cm (m³/m³)` - Rata-rata kelembapan air pada tanah untuk kedalaman 28-100 cm
* `soil_moisture_100_to_255cm (m³/m³)` - Rata-rata kelembapan air pada tanah untuk kedalaman 100-255 cm
* `city` - Nama kota

## Reading Data
"""

# Import library yang dibutuhkan
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns
import re

from numpy import where
from matplotlib import pyplot
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score

from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression, ElasticNet, HuberRegressor, RANSACRegressor, TheilSenRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

from sklearn.preprocessing import LabelEncoder, Normalizer, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV, GridSearchCV, StratifiedKFold

from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks

# Download datasets
!gdown 1kcb-zkYu-UzAcLM2VfTBgZ8MsIvpdHlK
!gdown 14mTR39Y6NhFfAwz6m7q3JjAKJPQnmFlI
!gdown 1qRuVVEDUwjtZy_nzXfCOWXn5XdrhghHW
!gdown 1C7Ly2ZLslpzRnoS1rYXEXFtCcAJ_Rldm

"""### Data Train"""

# Convert csv to data frame.
df_train_harian = pd.read_csv('train_harian.csv')
df_train_harian.head()

df_train_harian.shape

# Convert csv to data frame.
df_train_jaman = pd.read_csv('train_jaman.csv')
df_train_jaman.head()

df_train_jaman.shape

#Menggabungkan dua dataset
merged_df = pd.merge(df_train_harian, df_train_jaman, how='inner')

"""###Data Test"""

# Convert csv to data frame.
df_test_harian = pd.read_csv('test_harian.csv')
df_test_harian.head()

df_test_harian.shape

# Convert csv to data frame.
df_test_jaman = pd.read_csv('test_jaman.csv')
df_test_jaman.head()

df_test_jaman.shape

#Menggabungkan dua dataset
merged_test = pd.merge(df_test_harian, df_test_jaman, how='inner')

"""## Exploratory Data Analysis (EDA)

### 1. Bagaimana hubungan ke empat kolom mengenai `soil_moisture` dengan jumlah curah hujan harian?
"""

#Melihat Korelasi Menggunakan Scatterplot
soil_moisture_cols = ['soil_moisture_0_to_7cm (m³/m³)', 'soil_moisture_7_to_28cm (m³/m³)', 'soil_moisture_28_to_100cm (m³/m³)', 'soil_moisture_100_to_255cm (m³/m³)']
titles = ['Hubungan Soil Moisture 0 to 7cm (m³/m³) Dengan Jumlah Curah Hujan Harian',
          'Hubungan Soil Moisture 7 to 28cm (m³/m³) Dengan Jumlah Curah Hujan Harian',
          'Hubungan Soil Moisture 28 to 100cm (m³/m³) Dengan Jumlah Curah Hujan Harian',
          'Hubungan Soil Moisture 100 to 255cm (m³/m³) Dengan Jumlah Curah Hujan Harian']

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

axes = axes.flatten()

for i, variable in enumerate(soil_moisture_cols):
    sns.scatterplot(x=variable, y='rain_sum (mm)', data=merged_df, ax=axes[i])
    axes[i].set_xlabel(variable)
    axes[i].set_ylabel('Jumlah Curah Hujan Harian')
    axes[i].set_title(titles[i])
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()

plt.show()

columns = ['soil_moisture_0_to_7cm (m³/m³)', 'soil_moisture_7_to_28cm (m³/m³)', 'soil_moisture_28_to_100cm (m³/m³)', 'soil_moisture_100_to_255cm (m³/m³)', 'rain_sum (mm)']
merged_df[columns].corr(method='pearson')

"""> Dari scatterplot dan tabel korelasi di atas, dapat kita ambil kesimpulan bahwa keempat kolom soil moisture memiliki hubungan korelasi yang kuat satu sama lain. Hal ini tidak kita inginkan dalam pelatihan model, karena akan membuat informasi menjadi redundan dan dapat menyebabkan overfit. Oleh karena itu, apabila ingin menggunakan soil moisture sebagai fitur, cukup kita pilih salah satu.  
Adapun hubungan antara keempat kolom soil moisture dengan curah hujan terbilang cukup rendah, sehingga perlu kita pertimbangkan apakah akan menggunakan kolom soil moisture sebagai fitur.

### 2. Apakah ada beberapa kota yang dekat secara geografisnya?
"""

merged_df.info()

merged_df.head()

"""> Dengan dugaan awal bahwa kedekatan geografis sebuah kota dapat dilihat dari waktu terbitnya matahari yang berdekatan, maka kita buat sebuah dataframe baru yang berisi informasi kota dan waktu terbitnya matahari."""

#Melihat kota - kota yang tercatat pada dataset
unique_city_harian = df_train_harian['city'].unique()
print(unique_city_harian)

sunrise_df = merged_df[['sunrise (iso8601)', 'city']]
sunrise_df = sunrise_df[sunrise_df['sunrise (iso8601)'] > '2023-04-10'].sort_values(by='sunrise (iso8601)', ascending=False)
sunrise_df

plt.figure(figsize=(20,10))
plt.scatter(y=sunrise_df['sunrise (iso8601)'], x=sunrise_df['city'])
plt.show()

"""> Kita lihat dari scatter plot di atas, bahwa terdapat beberapa kota yang waktu matahari terbitnya berdekatan, diantaranya:
1. Kota P dengan kota U selisih +- 10 menit
2. Kota U dengan kota S selisih +- 1 menit
3. Kota L dengan kota M selisih +- 10 menit  

>Dengan demikian, bisa jadi beberapa kota yang disebutkan di atas memiliki letak geografis yang berdekatan.

### 4. Eksplorasi 3 kota yang memiliki rata-rata jumlah curah hujan tahunan tertinggi beserta nilainya!
"""

# Ambil data curah hujan, kota, dan wa
curah_hujan_per_kota = merged_df[['time','rain_sum (mm)', 'city']]
curah_hujan_per_kota

# ganti atribut time dengan informasi tahun saja
curah_hujan_per_kota['year'] = pd.DatetimeIndex(curah_hujan_per_kota['time']).year

# drop time
curah_hujan_per_kota = curah_hujan_per_kota.drop('time', axis=1)

curah_hujan_per_kota.groupby(['city', 'year']).mean()

# menghitung rata2 curah hujan tahunan tiap kota
# hitung rata2 curah hujan per tahun terlebih dahulu, kemudian baru hitung rata2 curah hujan tahunan
curah_hujan_per_kota.groupby(['city', 'year']).mean().groupby(['city']).mean().sort_values(by='rain_sum (mm)', ascending=False)

cities = ['T', 'M', 'Kw', 'P', 'S', 'B', 'U', 'L', 'Sh', 'K']
rainfall_values = [21.882771, 20.135069, 8.628017, 8.327139, 7.726099, 5.575711, 4.824707, 3.881636, 3.639435, 1.234052]

# Create a line chart
plt.figure(figsize=(10, 6))
plt.plot(cities, rainfall_values, marker='o', linestyle='-')

# Add labels and title
plt.title('Average Annual Rainfall per City')
plt.xlabel('City')
plt.ylabel('Average Rainfall (mm)')

# Display the values on each data point
for i, txt in enumerate(rainfall_values):
    plt.annotate(f'{txt:.2f}', (cities[i], rainfall_values[i]), textcoords="offset points", xytext=(0, 10), ha='center')

# Show the plot
plt.grid(True)
plt.show()

"""> Dari hasil perhitungan di atas didapat bahwa 3 kota dengan curah hujan tahunan tertinggi adalah:
1. Kota T (21.882)
2. Kota M (20.135)
3. Kota Kw (8.628)

### 5. Rata - Rata Snowfall Berdasarkan City
"""

# Analysis
df_p = merged_df.copy()

p_by_city = df_p.groupby('city', as_index=False)['snowfall (cm)'].mean()
p_by_city

# Visualization
ax = sns.barplot(
    data=p_by_city,
    x='snowfall (cm)',
    y='city',
    errorbar=None,
)
for container in ax.containers:
    ax.bar_label(container)
plt.show()

"""Terlihat bahwa kota yang memiliki nilai snowfall adalah kota K dan Sh dengan nilai snowfall (cm) tertinggi ada pada kota K

### 6. Rata - Rata Surface Pressure Berdasarkan Kota
"""

# Analysis
df_rs = merged_df.copy()

rs_by_city = df_rs.groupby('city', as_index=False)['surface_pressure (hPa)'].mean()
rs_by_city

# Visualization
ax = sns.barplot(
    data=rs_by_city,
    x='surface_pressure (hPa)',
    y='city',
    errorbar=None,
)
for container in ax.containers:
    ax.bar_label(container)
plt.show()

"""Dari bar chart di atas, terlihat bahwa rata - rata surface pressure tertinggi berada di kota Sh.

###7. Hubungan Antara Temperatur dengan Kota Pilihan Terhadap Jumlah Curah Hujan
"""

df_temp = merged_df.copy()

selected_cities = ['T', 'M', 'Kw']

df_temp = df_temp[df_temp['city'].isin(selected_cities)]

# Visualization
ax = sns.scatterplot(
    data=df_temp[['city', 'temperature_2m (°C)','rain_sum (mm)']],
    x="rain_sum (mm)", y="temperature_2m (°C)",
    hue="city"
)

sns.move_legend(
    ax, "lower center",
    bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,
)

"""## Data Preprocessing

### Missing Values

#### Check Missing Values
"""

def check_values(df):
    data = []
    for col in df.columns:
        data.append([col, \
                  df[col].dtype, \
                  df[col].isna().sum(), \
                  round(100*(df[col].isna().sum()/len(df)), 2), \
                  df[col].nunique(), \
                  df[col].unique()
                  ])

    return pd.DataFrame(columns=['kolom', 'dataType', 'null', 'nullPercentage', 'unique', 'values'], data=data)

check_values(merged_df)

check_values(merged_test)

"""#### Handle Missing Values"""

# Menghapus atribut pada data train yang memiliki nilai null dalam jumlah besar
merged_df.drop(['precipitation' , 'snowfall'], axis=1, inplace=True)

# Menghapus atribut pada data test yang memiliki nilai null dalam jumlah besar
merged_test.drop(['precipitation' , 'snowfall'], axis=1, inplace=True)

# Mengecek nilai null
def check_null(df):
    col_na = df.isnull().sum().sort_values(ascending=True)
    percent = col_na / len(df)
    missing_data = pd.concat([col_na, percent], axis=1, keys=['Total', 'Percent'])

    if (missing_data[missing_data['Total'] > 0].shape[0] == 0):
        print("Tidak ditemukan missing value pada dataset")
    else:
        print(missing_data[missing_data['Total'] > 0])

check_null(merged_df)

check_null(merged_test)

"""### Duplicate Values

#### Check Duplicate Values
"""

#Mengecek Nilai Duplikat pada data train
merged_df.duplicated()
print("Jumlah duplikasi data : " + str(merged_df.duplicated().sum()))

#Mengecek Nilai Duplikat pada data test
merged_test.duplicated()
print("Jumlah duplikasi data : " + str(merged_df.duplicated().sum()))

"""### Outliers

#### Check Outliers
"""

num_df = merged_df.select_dtypes(include=['int64', 'float64'])
print(num_df.columns)

num_cols = 3
num_rows = (len(num_df.columns) + num_cols - 1) // num_cols

fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20))
axes = axes.flatten()

for i, column in enumerate(num_df.columns):
    axes[i].boxplot(num_df[column])
    axes[i].set_title(f"Box Plot of {column}")
    axes[i].set_xlabel(column)
    axes[i].set_ylabel("Values")

if len(num_df.columns) < num_rows * num_cols:
    for j in range(len(num_df.columns), num_rows * num_cols):
        fig.delaxes(axes[j])

fig.tight_layout()
plt.show()

def check_outliers(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)

    # Menghitung RUB dan RLB.
    IQR = Q3 - Q1
    lower_limit = Q1 - 1.5*IQR
    upper_limit = Q3 + 1.5*IQR

    # Menampilkan banyaknya outlier pada atribut.
    outliers = (df < lower_limit) | (df > upper_limit)
    print ("Outlier pada tiap atribut:")
    print(outliers.sum())

    return outliers

df_outliers = check_outliers(merged_df.select_dtypes(include=[np.number]))
print("Tabel feature beserta jumlah outliernya dengan persentase kolom:\n")
df_outliers

"""#### Handle Ouliers"""

#Do Nothing to Handling Outliers

"""### Unwantend Words

#### Handle Unwanted Words
"""

#Menghapus kata yang ambigu pada values agar seragam
def remove_unwanted_words(temperature_2m_max):
    unwanted_words = ['drjt', 'derajat', 'celcius']
    replacement_words = ['', '', '']

    for i in range(len(unwanted_words)):
        temperature_2m_max = temperature_2m_max.replace(unwanted_words[i], replacement_words[i])

    return temperature_2m_max

merged_df['temperature_2m_max (°C)'] = merged_df['temperature_2m_max (°C)'].apply(lambda x: ', '.join(sorted(set([temperature_2m_max.strip().lower() for temperature_2m_max in x.split(',')]))))
merged_df['temperature_2m_max (°C)'] = merged_df['temperature_2m_max (°C)'].apply(remove_unwanted_words)

unique_names = merged_df['class'].unique()

print(unique_names)

unique_names = merged_df['temperature_2m_max (°C)'].unique()

print(unique_names)

"""###Data Transformation

#### Transform Datatype
"""

# Transformasi tipe data kolom time menjadi datetime
merged_df['time'] = pd.to_datetime(merged_df['time'])
merged_df['sunrise (iso8601)'] = pd.to_datetime(merged_df['sunrise (iso8601)'])
merged_df['sunset (iso8601)'] = pd.to_datetime(merged_df['sunset (iso8601)'])

# Transformasi tipe data kolom time menjadi datetime
merged_test['time'] = pd.to_datetime(merged_test['time'])
merged_test['sunrise (iso8601)'] = pd.to_datetime(merged_test['sunrise (iso8601)'])
merged_test['sunset (iso8601)'] = pd.to_datetime(merged_test['sunset (iso8601)'])

# Transformasi tipe data kolom 'temperatur' menjadi float
merged_df['temperature_2m_max (°C)'] = merged_df['temperature_2m_max (°C)'].astype(float)

check_values(merged_df)

check_values(merged_test)

"""### Splitting Datetime"""

from sklearn.metrics import precision_score, \
    recall_score, classification_report, \
    accuracy_score, f1_score

# Membuat fungsi untuk mengevaluasi performa model
def evaluate_classifier_performance(prediction, y_test):
    # Informasi evaluasi secara compact
    print("Hasil Evaluasi berdasarkan classification report \n\n%s\n" % (classification_report(y_test, prediction,zero_division=0)))
    print()
    print("Confusion Matrix")
    print()
    y_actual = pd.Series(np.array(y_test), name = "actual")
    y_pred = pd.Series(np.array(prediction), name = "prediction")
    df_confusion = pd.crosstab(y_actual, y_pred)
    display(df_confusion)
    print()
    print()

    print("Butuh informasi lebih lengkap? silakan simak di bawah ini : ")
    print('Accuracy Average:', accuracy_score(y_test, prediction))
    print('F1 Macro Average:', f1_score(y_test, prediction, average='macro'))
    print('F1 Micro Average:', f1_score(y_test, prediction, average='micro'))
    print('Precision Macro Average:', precision_score(y_test, prediction, average='macro',zero_division=0))
    print('Precision Micro Average:', precision_score(y_test, prediction, average='micro',zero_division=0))
    print('Recall Macro Average:', recall_score(y_test, prediction, average='macro',zero_division=0))
    print('Recall Micro Average:', recall_score(y_test, prediction, average='micro',zero_division=0))
    print()

def split_datetime(df):
  for (columnName, columnData) in df.items():
    if columnData.dtype == 'datetime64[ns]':
      df[columnName + '_year'] = df[columnName].dt.year
      df[columnName + '_month'] = df[columnName].dt.month
      df[columnName + '_day'] = df[columnName].dt.day

      # Drop the original datetime features.
      dropped_dates = [
          'time',
          'sunset (iso8601)',
          'sunrise (iso8601)'
      ]

      df.drop(dropped_dates, axis=1, inplace = True)

  return df

# Split datetime features harian to new columns by year, month, day, hour, minute and second.
# df_model_test = merged_test.copy()

# df_model_test['time_year'] = df_model_test['time'].dt.year
# df_model_test['time_month'] = df_model_test['time'].dt.month
# df_model_test['time_day'] = df_model_test['time'].dt.day

# df_model_test['sunrise_year'] = df_model_test['sunrise (iso8601)'].dt.year
# df_model_test['sunrise_month'] = df_model_test['sunrise (iso8601)'].dt.month
# df_model_test['sunrise_day'] = df_model_test['sunrise (iso8601)'].dt.day

# df_model_test['sunset_year'] = df_model_test['sunset (iso8601)'].dt.year
# df_model_test['sunset_month'] = df_model_test['sunset (iso8601)'].dt.month
# df_model_test['sunset_day'] = df_model_test['sunset (iso8601)'].dt.day

try:
  df_model = split_datetime(merged_df)
except:
  df_model = split_datetime(merged_df)

try:
  df_model_test = split_datetime(merged_test)
except:
  df_model_test = split_datetime(merged_test)

"""### Encoding of Categorical Features"""

# Encoding of categorical features.
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df_model['city'] = le.fit_transform(df_model['city'])
df_model['class'] = le.fit_transform(df_model['class'])

"""###Split Data Train and Test"""

# Spliting data to training and testing.
from sklearn.model_selection import train_test_split

x_regression = df_model.drop(['class', 'rain_sum (mm)', 'id'], axis=1)
y_regression = df_model['rain_sum (mm)']

x_classification = df_model.drop(['class', 'rain_sum (mm)','id'], axis=1)
y_classification = df_model['class']

# Spliting data to training and testing.
from sklearn.model_selection import train_test_split

x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x_regression, y_regression, test_size = 0.20, random_state = 42)
x_train_clas, x_test_clas, y_train_clas, y_test_clas = train_test_split(x_classification, y_classification, test_size = 0.20, random_state = 42)

"""### Imbalanced Data"""

sns.countplot(x='class', data=df_model)

"""#### Oversampling"""

from imblearn.over_sampling import BorderlineSMOTE
from collections import Counter

# Inisiasi oversampler dengan metode Borderline SMOTE
borderline_smote = BorderlineSMOTE()

# Melakukan resampling
X_borderline_smote, y_borderline_smote = borderline_smote.fit_resample(x_train_clas, y_train_clas)

# Melihat distribusi kelas pada dataset
counter_y_borderline_smote = Counter(y_borderline_smote)
print(counter_y_borderline_smote)

"""###Feature Selection (Filter)

#### Classification SFS
"""

from sklearn.linear_model import RidgeCV

ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(x_train_clas, y_train_clas)
importance = np.abs(ridge.coef_)
feature_names = np.array(X_borderline_smote.columns)
plt.figure(figsize=(20, 6))
plt.bar(height=importance, x=feature_names)
plt.title("Feature importances via coefficients")
plt.show()

from sklearn.feature_selection import SequentialFeatureSelector

sfs_forward_clas = SequentialFeatureSelector(
    ridge, n_features_to_select=10, direction="forward"
).fit(x_train_clas, y_train_clas)

sfs_backward_clas = SequentialFeatureSelector(
    ridge, n_features_to_select=10, direction="backward"
).fit(x_train_clas, y_train_clas)

print(
    "Features selected by forward sequential selection: "
    f"{feature_names[sfs_forward_clas.get_support()]}"
)
print(
    "Features selected by backward sequential selection: "
    f"{feature_names[sfs_backward_clas.get_support()]}"
)

from sklearn.preprocessing import StandardScaler
# scaling
scaler_clas = StandardScaler()
X_sffs_clas = scaler_clas.fit_transform(x_train_clas[feature_names[sfs_forward_clas.get_support()]])
X_sffs_clas

"""#### Regression SFS"""

from sklearn.linear_model import RidgeCV

ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(x_train_reg, y_train_reg)
importance = np.abs(ridge.coef_)
feature_names = np.array(x_train_reg.columns)
plt.figure(figsize=(20, 6))
plt.bar(height=importance, x=feature_names)
plt.title("Feature importances via coefficients")
plt.show()

from sklearn.feature_selection import SequentialFeatureSelector

sfs_forward = SequentialFeatureSelector(
    ridge, n_features_to_select=8, direction="forward"
).fit(x_train_reg, y_train_reg)

sfs_backward = SequentialFeatureSelector(
    ridge, n_features_to_select=8, direction="backward"
).fit(x_train_reg, y_train_reg)

print(
    "Features selected by forward sequential selection: "
    f"{feature_names[sfs_forward.get_support()]}"
)
print(
    "Features selected by backward sequential selection: "
    f"{feature_names[sfs_backward.get_support()]}"
)

from sklearn.preprocessing import StandardScaler
# scaling
scaler_reg = StandardScaler()
X_sffs = scaler_reg.fit_transform(x_train_reg[feature_names[sfs_forward.get_support()]])
X_sffs

"""## Classification

###Random Forest
"""

# x_test_clas_ready = scaler_pca.transform(x_test_clas)
# x_test_clas_ready = pca.transform(x_test_clas_ready)

x_test_clas_ready = scaler_clas.fit_transform(x_test_clas[feature_names[sfs_forward_clas.get_support()]])

X_sffs_clas.shape

from sklearn.ensemble import RandomForestClassifier

# Training of random forest model.
rf_clas = RandomForestClassifier()
rf_clas.fit(X_sffs_clas, y_train_clas)

# Predicting of test data.
y_pred_clas = rf_clas.predict(x_test_clas_ready)

# Evaluation of random forest model.
evaluate_classifier_performance(y_pred_clas, y_test_clas)

"""### Predict Classification Test Data"""

df_model_test['city'] = le.fit_transform(df_model_test['city'])
classification_X = scaler_clas.transform(df_model_test[feature_names[sfs_forward_clas.get_support()]])

# Melakukan prediksi
classification_pred = rf_clas.predict(classification_X)
classification_pred

"""## Regression

### Random Forest Regressor
"""

# x_test_reg_ready = scaler_pca.transform(x_test_reg)
# x_test_reg_ready = pca.transform(x_test_reg_ready)

x_test_reg_ready = scaler_reg.fit_transform(x_test_reg[feature_names[sfs_forward.get_support()]])

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Membuat model regresi linear
rf_reg = RandomForestRegressor()

# Melatih model pada data training yang telah distandarisasi
rf_reg.fit(X_sffs, y_train_reg)

# Melakukan prediksi pada data testing yang telah distandarisasi
y_pred_reg = rf_reg.predict(x_test_reg_ready)

# Menampilkan hasil prediksi pada keseluruhan data uji
print("Hasil Prediksi pada Keseluruhan Data Uji:")
df_test = pd.DataFrame({'Actual': y_test_reg, 'Predicted': y_pred_reg})
df_test

# Mengukur performa model
mse = mean_squared_error(y_test_reg, y_pred_reg)
r2 = r2_score(y_test_reg, y_pred_reg)

print("\nMetrik Performa Model:")
print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')

"""### Predict Test Data"""

df_model_test['city'] = le.fit_transform(df_model_test['city'])
regression_X = scaler_reg.transform(df_model_test[feature_names[sfs_forward.get_support()]])

# Melakukan prediksi
regression_pred = rf_reg.predict(regression_X)
regression_pred

"""##Clustering"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from yellowbrick.cluster import SilhouetteVisualizer
import matplotlib.cm as cm
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA

X = df_model[['snowfall (cm)', 'relativehumidity_2m (%)', 'et0_fao_evapotranspiration (mm)']]

kmeans = KMeans(n_clusters=10)

cluster_assignment = kmeans.fit_predict(X)
data_with_clusters = pd.DataFrame(X.copy(), columns=('snowfall (cm)', 'relativehumidity_2m (%)', 'et0_fao_evapotranspiration (mm)'))
data_with_clusters['Clusters'] = cluster_assignment
data_with_clusters

# Create figure
fig = plt.figure(figsize = (15, 10))
ax = plt.axes(projection ="3d")

# Prepare data
x = data_with_clusters['snowfall (cm)']
y = data_with_clusters['relativehumidity_2m (%)']
z = data_with_clusters['et0_fao_evapotranspiration (mm)']
cluster = data_with_clusters['Clusters']

# Create plot
ax.scatter(x, y, z, c = cluster, cmap = "rainbow")
plt.title("Clusters")
ax.set_xlabel('snowfall (cm)')
ax.set_ylabel('relativehumidity_2m (%)')
ax.set_zlabel('et0_fao_evapotranspiration (mm)')

# Show plot
plt.show()

"""##Submission

###Classification
"""

mapping = {
    0 : 'Hujan Lebat',
    1 : 'Hujan Ringan',
    2 : 'Hujan Sangat Lebat',
    3 : 'Hujan Lebat',
    4 : 'Tidak Hujan'
}

string_array = [mapping[value] for value in classification_pred]

submit_classification = pd.DataFrame({'id': df_model_test['id'],
                   'class': string_array})

submit_classification.to_csv('MC_Classification.csv', sep=',', index=False, encoding='utf-8')

"""###Regression"""

submit_regression = pd.DataFrame({'id': merged_test['id'],
                   'rain_sum (mm)': regression_pred})

submit_regression.to_csv('MC_regression.csv', sep=',', index=False, encoding='utf-8')